# ETL Local (Desafio Localiza)

Solu√ß√£o desenvolvida para o desafio t√©cnico de Engenharia de Dados, com foco em simplicidade, qualidade e reprodutibilidade.

# üõ†Ô∏è Arquitetura

- Orquestra√ß√£o ‚Üí Prefect 2 (UI em http://localhost:4200)
- Camada anal√≠tica ‚Üí DuckDB (banco colunar leve, em arquivo √∫nico .duckdb)
- Transforma√ß√µes ‚Üí SQL dentro do DuckDB
- Entrega ‚Üí Resultados exportados em CSV

## Estrutura
```
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ Dockerfile
‚îú‚îÄ requirements.txt
‚îú‚îÄ flows/pipeline.py
‚îú‚îÄ input/df_fraud_credit.csv    # dataset de entrada (n√£o versionado)
‚îú‚îÄ data/results.duckdb          # banco gerado
‚îî‚îÄ curated/
   ‚îú‚îÄ region_risk_avg.csv
   ‚îî‚îÄ top3_recent_sales_by_receiving.csv
```

## Como rodar (Docker)
1. Coloque seu arquivo na pasta `input/` com nome `df_fraud_credit.csv` (ou ajuste a env var abaixo).
2. Construa e execute:
   ```bash
   docker compose up --build
   ```
   O container roda o pipeline e termina. Os artefatos ficam nas pastas montadas.

## Como rodar (local sem Docker)
```bash
pip install -r requirements.txt
python flows/pipeline.py
```

## O que o pipeline faz
1. **Ingest√£o local**: l√™ `./input/df_fraud_credit.csv`.
2. **Limpeza e padroniza√ß√£o**:
   - `timestamp` ‚Üí datetime (UTC); inv√°lidos s√£o descartados.
   - `amount` ‚Üí num√©rico; negativos s√£o tratados como inv√°lidos (descartados).
   - `transaction_type` ‚Üí normalizado para min√∫sculas (e.g., `sale`, `refund`).
   - `receiving_address` e `location_region` ‚Üí `.str.strip()`, vazios/"0" viram `NaN`.
   - **Dedupe** por (`timestamp`,`receiving_address`,`transaction_type`,`amount`).
3. **Data Quality (automatizado)**: valida√ß√µes m√≠nimas
   - Duas verifica√ß√µes s√£o realizadas:
     1. **Camada raw**: valida√ß√µes no arquivo original, sem normaliza√ß√£o ou tratamento.
   - Regras aplicadas:
     - Conformidade m√≠nima de 98%; abaixo disso o job sai com c√≥digo 2 (falha). Escrevendo a camada raw sem tratamentos para analises de erro
   - M√©tricas em `data/dq_metrics_pre.json`.
     2. **Ap√≥s limpeza**: valida√ß√µes no dataset tratado.
   - Regras aplicadas:
     - `timestamp`, `transaction_type`, `amount` n√£o nulos.
     - `amount >= 0`. 
     - Conformidade m√≠nima de 99.5%; abaixo disso o job sai com c√≥digo 2 (falha). Escrevendo a camada tratada para analises de erro.
   - M√©tricas em `data/dq_metrics_post.json`.
4. **Transforma√ß√µes (DuckDB)**:
   - Tabela `stg_transactions` (limpa) em `data/results.duckdb`.
   - **Resultado 1**: `region_risk_avg` (m√©dia de `risk_score` por `location_region`, desc.).
   - **Resultado 2**: `top3_recent_sales_by_receiving` (para cada `receiving_address`, pega a *√∫ltima* transa√ß√£o `sale` por `timestamp` e lista o top 3 por `amount` desc.).
   - Exporta resultados para `./curated` em **CSV**.

## Vari√°veis de ambiente
- `INPUT_CSV` (default: `./input/df_fraud_credit.csv`)

## Arquitetura do Pipeline

Abaixo est√° a arquitetura do pipeline de ETL:

![Arquitetura do Pipeline](docs/Arquitetura.svg)

